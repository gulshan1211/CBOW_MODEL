{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "sentences={\n",
        "  \"sentences\": [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog barked at the cat.\",\n",
        "    \"The cat chased the mouse.\",\n",
        "    \"The dog ran after the ball.\",\n",
        "    \"The cat napped in the sun.\",\n",
        "    \"The dog dug a hole in the yard.\",\n",
        "    \"The cat climbed the tree.\",\n",
        "    \"The dog fetched the stick.\",\n",
        "    \"The cat meowed loudly.\",\n",
        "    \"The dog wagged its tail.\",\n",
        "    \"The cat scratched the post.\",\n",
        "    \"The dog rolled over.\",\n",
        "    \"The cat licked its paws.\",\n",
        "    \"The dog jumped over the fence.\",\n",
        "    \"The cat purred softly.\",\n",
        "    \"The dog sniffed the ground.\",\n",
        "    \"The cat stretched lazily.\",\n",
        "    \"The dog chewed the bone.\",\n",
        "    \"The cat hid under the bed.\",\n",
        "    \"The dog barked at the mailman.\",\n",
        "    \"The cat watched the bird.\",\n",
        "    \"The dog chased its tail.\",\n",
        "    \"The cat yawned and slept.\",\n",
        "    \"The dog howled at the moon.\",\n",
        "    \"The cat groomed itself.\",\n",
        "    \"The dog drank water from the bowl.\",\n",
        "    \"The cat played with the yarn.\",\n",
        "    \"The dog ran in the park.\",\n",
        "    \"The cat observed the fish.\",\n",
        "    \"The dog lay on the couch.\",\n",
        "    \"The cat blinked slowly.\",\n",
        "    \"The dog barked at strangers.\",\n",
        "    \"The cat followed the laser.\",\n",
        "    \"The dog guarded the house.\",\n",
        "    \"The cat napped on the windowsill.\",\n",
        "    \"The dog played with the toy.\",\n",
        "    \"The cat scratched the sofa.\",\n",
        "    \"The dog sniffed the air.\",\n",
        "    \"The cat chased a butterfly.\",\n",
        "    \"The dog dug in the garden.\",\n",
        "    \"The cat sat by the window.\",\n",
        "    \"The dog ran around the yard.\",\n",
        "    \"The cat meowed for food.\",\n",
        "    \"The dog lay in the sun.\",\n",
        "    \"The cat napped on the chair.\",\n",
        "    \"The dog barked at the squirrel.\",\n",
        "    \"The cat climbed onto the roof.\",\n",
        "    \"The dog fetched the newspaper.\",\n",
        "    \"The cat chased its tail.\",\n",
        "    \"The dog slept by the door.\",\n",
        "    \"The cat watched the rain.\",\n",
        "    \"The dog played in the mud.\",\n",
        "    \"The cat sat on the fence.\",\n",
        "    \"The dog chased the car.\",\n",
        "    \"The cat licked the milk.\",\n",
        "    \"The dog wagged its tail happily.\",\n",
        "    \"The cat jumped onto the shelf.\",\n",
        "    \"The dog sniffed the flowers.\",\n",
        "    \"The cat purred in contentment.\",\n",
        "    \"The dog ran along the beach.\",\n",
        "    \"The cat meowed at the door.\",\n",
        "    \"The dog barked at the moon.\",\n",
        "    \"The cat napped on the couch.\",\n",
        "    \"The dog chased the shadow.\",\n",
        "    \"The cat climbed the curtains.\",\n",
        "    \"The dog fetched the ball.\",\n",
        "    \"The cat hid in the closet.\",\n",
        "    \"The dog ran through the field.\",\n",
        "    \"The cat watched the sunset.\",\n",
        "    \"The dog lay on the porch.\",\n",
        "    \"The cat yawned and stretched.\",\n",
        "    \"The dog barked at the gate.\",\n",
        "    \"The cat played with the feather.\",\n",
        "    \"The dog sniffed the grass.\",\n",
        "    \"The cat climbed the bookshelf.\",\n",
        "    \"The dog wagged its tail excitedly.\",\n",
        "    \"The cat napped in the basket.\",\n",
        "    \"The dog ran after the frisbee.\",\n",
        "    \"The cat observed the butterfly.\",\n",
        "    \"The dog barked at the bird.\",\n",
        "    \"The cat licked the fur.\",\n",
        "    \"The dog lay on the floor.\",\n",
        "    \"The cat played with the toy mouse.\",\n",
        "    \"The dog fetched the stick happily.\",\n",
        "    \"The cat watched the TV.\",\n",
        "    \"The dog dug a hole in the garden.\",\n",
        "    \"The cat purred in the lap.\",\n",
        "    \"The dog sniffed the shoes.\",\n",
        "    \"The cat napped on the rug.\",\n",
        "    \"The dog chased the bicycle.\",\n",
        "    \"The cat meowed at the bird.\",\n",
        "    \"The dog played with the ball.\",\n",
        "    \"The cat watched the butterfly.\",\n",
        "    \"The dog ran through the park.\",\n",
        "    \"The cat napped in the box.\",\n",
        "    \"The dog barked at the noise.\",\n",
        "    \"The cat hid behind the curtain.\",\n",
        "    \"The dog wagged its tail joyfully.\",\n",
        "    \"The cat observed the surroundings.\",\n",
        "    \"The dog lay on the grass.\"\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "bDABaT0RLG4j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5KXQmRCG5sj",
        "outputId": "bb814b44-ad9d-41b5-dcc7-c8b51307d0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Define a set of pronouns to remove\n",
        "pronouns = {'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
        "            'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him',\n",
        "            'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its',\n",
        "            'itself', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
        "\n",
        "def preprocess_question(question):\n",
        "    # Remove punctuation and special characters\n",
        "    question = re.sub(r'[^\\w\\s]', '', question)\n",
        "    # Convert to lowercase\n",
        "    question = question.lower()\n",
        "    # Tokenize the question\n",
        "    question_words = question.split()\n",
        "    pos_tags = nltk.pos_tag(question_words)\n",
        "    lemmas = []\n",
        "    for word, tag in pos_tags:\n",
        "        wn_tag = get_wordnet_pos(tag)\n",
        "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "        lemmas.append(lemma)\n",
        "    # Remove stop words and pronouns\n",
        "    filtered_words = [word for word in lemmas if word not in stop_words and word not in pronouns]\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n' # default to noun if tag is not found"
      ],
      "metadata": {
        "id": "j529XCtqH83_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "GDIw4Y_6JmTo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_questions = [preprocess_question(q) for q in sentences['sentences']]"
      ],
      "metadata": {
        "id": "MnrRIxXdLuxL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVXUTVUBNU8T",
        "outputId": "ca9e2f08-5c29-4d1c-c80e-d9d4c07c375f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat sit mat',\n",
              " 'dog bark cat',\n",
              " 'cat chase mouse',\n",
              " 'dog run ball',\n",
              " 'cat nap sun',\n",
              " 'dog dig hole yard',\n",
              " 'cat climb tree',\n",
              " 'dog fetch stick',\n",
              " 'cat meow loudly',\n",
              " 'dog wag tail',\n",
              " 'cat scratch post',\n",
              " 'dog roll',\n",
              " 'cat lick paw',\n",
              " 'dog jump fence',\n",
              " 'cat purr softly',\n",
              " 'dog sniff ground',\n",
              " 'cat stretch lazily',\n",
              " 'dog chew bone',\n",
              " 'cat hid bed',\n",
              " 'dog bark mailman',\n",
              " 'cat watch bird',\n",
              " 'dog chase tail',\n",
              " 'cat yawn sleep',\n",
              " 'dog howl moon',\n",
              " 'cat groom',\n",
              " 'dog drank water bowl',\n",
              " 'cat play yarn',\n",
              " 'dog run park',\n",
              " 'cat observe fish',\n",
              " 'dog lay couch',\n",
              " 'cat blink slowly',\n",
              " 'dog bark stranger',\n",
              " 'cat follow laser',\n",
              " 'dog guard house',\n",
              " 'cat nap windowsill',\n",
              " 'dog play toy',\n",
              " 'cat scratch sofa',\n",
              " 'dog sniff air',\n",
              " 'cat chase butterfly',\n",
              " 'dog dug garden',\n",
              " 'cat sit window',\n",
              " 'dog run around yard',\n",
              " 'cat meow food',\n",
              " 'dog lay sun',\n",
              " 'cat nap chair',\n",
              " 'dog bark squirrel',\n",
              " 'cat climb onto roof',\n",
              " 'dog fetch newspaper',\n",
              " 'cat chase tail',\n",
              " 'dog slept door',\n",
              " 'cat watch rain',\n",
              " 'dog play mud',\n",
              " 'cat sit fence',\n",
              " 'dog chase car',\n",
              " 'cat lick milk',\n",
              " 'dog wag tail happily',\n",
              " 'cat jump onto shelf',\n",
              " 'dog sniff flower',\n",
              " 'cat purr contentment',\n",
              " 'dog run along beach',\n",
              " 'cat meow door',\n",
              " 'dog bark moon',\n",
              " 'cat nap couch',\n",
              " 'dog chase shadow',\n",
              " 'cat climb curtain',\n",
              " 'dog fetch ball',\n",
              " 'cat hid closet',\n",
              " 'dog run field',\n",
              " 'cat watch sunset',\n",
              " 'dog lay porch',\n",
              " 'cat yawn stretch',\n",
              " 'dog bark gate',\n",
              " 'cat play feather',\n",
              " 'dog sniff grass',\n",
              " 'cat climb bookshelf',\n",
              " 'dog wag tail excitedly',\n",
              " 'cat nap basket',\n",
              " 'dog run frisbee',\n",
              " 'cat observe butterfly',\n",
              " 'dog bark bird',\n",
              " 'cat lick fur',\n",
              " 'dog lay floor',\n",
              " 'cat play toy mouse',\n",
              " 'dog fetch stick happily',\n",
              " 'cat watch tv',\n",
              " 'dog dig hole garden',\n",
              " 'cat purr lap',\n",
              " 'dog sniff shoe',\n",
              " 'cat nap rug',\n",
              " 'dog chase bicycle',\n",
              " 'cat meow bird',\n",
              " 'dog play ball',\n",
              " 'cat watch butterfly',\n",
              " 'dog run park',\n",
              " 'cat nap box',\n",
              " 'dog bark noise',\n",
              " 'cat hid behind curtain',\n",
              " 'dog wag tail joyfully',\n",
              " 'cat observe surroundings',\n",
              " 'dog lay grass']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_questions)"
      ],
      "metadata": {
        "id": "XPtla6UgNmeQ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.fit_on_texts(preprocessed_questions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ4EXp_MNoc2",
        "outputId": "88d06a71-17fa-4f03-9b6e-8529dc9c3e97"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXpDM9EKNywU",
        "outputId": "9c0fe78e-1333-4085-8eeb-982a1a9b3eea"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cat': 1,\n",
              " 'dog': 2,\n",
              " 'bark': 3,\n",
              " 'chase': 4,\n",
              " 'run': 5,\n",
              " 'nap': 6,\n",
              " 'tail': 7,\n",
              " 'play': 8,\n",
              " 'sniff': 9,\n",
              " 'watch': 10,\n",
              " 'lay': 11,\n",
              " 'climb': 12,\n",
              " 'fetch': 13,\n",
              " 'meow': 14,\n",
              " 'wag': 15,\n",
              " 'sit': 16,\n",
              " 'ball': 17,\n",
              " 'lick': 18,\n",
              " 'purr': 19,\n",
              " 'hid': 20,\n",
              " 'bird': 21,\n",
              " 'observe': 22,\n",
              " 'butterfly': 23,\n",
              " 'mouse': 24,\n",
              " 'sun': 25,\n",
              " 'dig': 26,\n",
              " 'hole': 27,\n",
              " 'yard': 28,\n",
              " 'stick': 29,\n",
              " 'scratch': 30,\n",
              " 'jump': 31,\n",
              " 'fence': 32,\n",
              " 'stretch': 33,\n",
              " 'yawn': 34,\n",
              " 'moon': 35,\n",
              " 'park': 36,\n",
              " 'couch': 37,\n",
              " 'toy': 38,\n",
              " 'garden': 39,\n",
              " 'onto': 40,\n",
              " 'door': 41,\n",
              " 'happily': 42,\n",
              " 'curtain': 43,\n",
              " 'grass': 44,\n",
              " 'mat': 45,\n",
              " 'tree': 46,\n",
              " 'loudly': 47,\n",
              " 'post': 48,\n",
              " 'roll': 49,\n",
              " 'paw': 50,\n",
              " 'softly': 51,\n",
              " 'ground': 52,\n",
              " 'lazily': 53,\n",
              " 'chew': 54,\n",
              " 'bone': 55,\n",
              " 'bed': 56,\n",
              " 'mailman': 57,\n",
              " 'sleep': 58,\n",
              " 'howl': 59,\n",
              " 'groom': 60,\n",
              " 'drank': 61,\n",
              " 'water': 62,\n",
              " 'bowl': 63,\n",
              " 'yarn': 64,\n",
              " 'fish': 65,\n",
              " 'blink': 66,\n",
              " 'slowly': 67,\n",
              " 'stranger': 68,\n",
              " 'follow': 69,\n",
              " 'laser': 70,\n",
              " 'guard': 71,\n",
              " 'house': 72,\n",
              " 'windowsill': 73,\n",
              " 'sofa': 74,\n",
              " 'air': 75,\n",
              " 'dug': 76,\n",
              " 'window': 77,\n",
              " 'around': 78,\n",
              " 'food': 79,\n",
              " 'chair': 80,\n",
              " 'squirrel': 81,\n",
              " 'roof': 82,\n",
              " 'newspaper': 83,\n",
              " 'slept': 84,\n",
              " 'rain': 85,\n",
              " 'mud': 86,\n",
              " 'car': 87,\n",
              " 'milk': 88,\n",
              " 'shelf': 89,\n",
              " 'flower': 90,\n",
              " 'contentment': 91,\n",
              " 'along': 92,\n",
              " 'beach': 93,\n",
              " 'shadow': 94,\n",
              " 'closet': 95,\n",
              " 'field': 96,\n",
              " 'sunset': 97,\n",
              " 'porch': 98,\n",
              " 'gate': 99,\n",
              " 'feather': 100,\n",
              " 'bookshelf': 101,\n",
              " 'excitedly': 102,\n",
              " 'basket': 103,\n",
              " 'frisbee': 104,\n",
              " 'fur': 105,\n",
              " 'floor': 106,\n",
              " 'tv': 107,\n",
              " 'lap': 108,\n",
              " 'shoe': 109,\n",
              " 'rug': 110,\n",
              " 'bicycle': 111,\n",
              " 'box': 112,\n",
              " 'noise': 113,\n",
              " 'behind': 114,\n",
              " 'joyfully': 115,\n",
              " 'surroundings': 116}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_questions)\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_questions)"
      ],
      "metadata": {
        "id": "D8WJ4QbVOFKn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp1WMYUZOGwC",
        "outputId": "18d3cb00-de75-4e46-eb9f-e9fe986e89a4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 16, 45],\n",
              " [2, 3, 1],\n",
              " [1, 4, 24],\n",
              " [2, 5, 17],\n",
              " [1, 6, 25],\n",
              " [2, 26, 27, 28],\n",
              " [1, 12, 46],\n",
              " [2, 13, 29],\n",
              " [1, 14, 47],\n",
              " [2, 15, 7],\n",
              " [1, 30, 48],\n",
              " [2, 49],\n",
              " [1, 18, 50],\n",
              " [2, 31, 32],\n",
              " [1, 19, 51],\n",
              " [2, 9, 52],\n",
              " [1, 33, 53],\n",
              " [2, 54, 55],\n",
              " [1, 20, 56],\n",
              " [2, 3, 57],\n",
              " [1, 10, 21],\n",
              " [2, 4, 7],\n",
              " [1, 34, 58],\n",
              " [2, 59, 35],\n",
              " [1, 60],\n",
              " [2, 61, 62, 63],\n",
              " [1, 8, 64],\n",
              " [2, 5, 36],\n",
              " [1, 22, 65],\n",
              " [2, 11, 37],\n",
              " [1, 66, 67],\n",
              " [2, 3, 68],\n",
              " [1, 69, 70],\n",
              " [2, 71, 72],\n",
              " [1, 6, 73],\n",
              " [2, 8, 38],\n",
              " [1, 30, 74],\n",
              " [2, 9, 75],\n",
              " [1, 4, 23],\n",
              " [2, 76, 39],\n",
              " [1, 16, 77],\n",
              " [2, 5, 78, 28],\n",
              " [1, 14, 79],\n",
              " [2, 11, 25],\n",
              " [1, 6, 80],\n",
              " [2, 3, 81],\n",
              " [1, 12, 40, 82],\n",
              " [2, 13, 83],\n",
              " [1, 4, 7],\n",
              " [2, 84, 41],\n",
              " [1, 10, 85],\n",
              " [2, 8, 86],\n",
              " [1, 16, 32],\n",
              " [2, 4, 87],\n",
              " [1, 18, 88],\n",
              " [2, 15, 7, 42],\n",
              " [1, 31, 40, 89],\n",
              " [2, 9, 90],\n",
              " [1, 19, 91],\n",
              " [2, 5, 92, 93],\n",
              " [1, 14, 41],\n",
              " [2, 3, 35],\n",
              " [1, 6, 37],\n",
              " [2, 4, 94],\n",
              " [1, 12, 43],\n",
              " [2, 13, 17],\n",
              " [1, 20, 95],\n",
              " [2, 5, 96],\n",
              " [1, 10, 97],\n",
              " [2, 11, 98],\n",
              " [1, 34, 33],\n",
              " [2, 3, 99],\n",
              " [1, 8, 100],\n",
              " [2, 9, 44],\n",
              " [1, 12, 101],\n",
              " [2, 15, 7, 102],\n",
              " [1, 6, 103],\n",
              " [2, 5, 104],\n",
              " [1, 22, 23],\n",
              " [2, 3, 21],\n",
              " [1, 18, 105],\n",
              " [2, 11, 106],\n",
              " [1, 8, 38, 24],\n",
              " [2, 13, 29, 42],\n",
              " [1, 10, 107],\n",
              " [2, 26, 27, 39],\n",
              " [1, 19, 108],\n",
              " [2, 9, 109],\n",
              " [1, 6, 110],\n",
              " [2, 4, 111],\n",
              " [1, 14, 21],\n",
              " [2, 8, 17],\n",
              " [1, 10, 23],\n",
              " [2, 5, 36],\n",
              " [1, 6, 112],\n",
              " [2, 3, 113],\n",
              " [1, 20, 114, 43],\n",
              " [2, 15, 7, 115],\n",
              " [1, 22, 116],\n",
              " [2, 11, 44]]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_questions)\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_questions)\n",
        "vocab_size = len(tokenizer.word_index)\n",
        "\n",
        "# Define parameters\n",
        "embedding_size = 1\n",
        "window_size = 1\n",
        "\n",
        "# Generate context-target pairs\n",
        "contexts = []\n",
        "targets = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    for i in range(window_size, len(sequence) - window_size):\n",
        "        context = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
        "        target = sequence[i]\n",
        "        contexts.append(context)\n",
        "        targets.append(target)"
      ],
      "metadata": {
        "id": "RONO3AA8Iahn"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=[]\n",
        "for context in contexts:\n",
        "  z=[0]*vocab_size;\n",
        "  for i in context:\n",
        "    z[i-1\n",
        "      ]=1\n",
        "  X.append(z)"
      ],
      "metadata": {
        "id": "7AiNytGkO8uf"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=[]\n",
        "for target in targets:\n",
        "  z=[0]*vocab_size;\n",
        "  z[target-1]=1\n",
        "  Y.append(z)"
      ],
      "metadata": {
        "id": "9N-lfuRMPa3K"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "# Define the model using tf.keras.Sequential\n",
        "model = Sequential([\n",
        "    Dense(2, input_shape=(vocab_size,)),\n",
        "    Activation('relu'),\n",
        "    Dense(vocab_size),\n",
        "    Activation('softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlhLLajWIJyJ",
        "outputId": "b6b00c3b-1d8a-440d-b2cd-6cef2fe07708"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 2)                 234       \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 116)               348       \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 116)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 582 (2.27 KB)\n",
            "Trainable params: 582 (2.27 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, epochs=20, batch_size=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo2aUbx2QvR5",
        "outputId": "51f071bb-b653-4eee-92a4-1797b3f1fb0b"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "28/28 [==============================] - 1s 2ms/step - loss: 4.7413 - accuracy: 0.0360 \n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.7119 - accuracy: 0.0360\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.6821 - accuracy: 0.0811\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.6501 - accuracy: 0.0811\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.6150 - accuracy: 0.0631\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.5737 - accuracy: 0.0631\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.5265 - accuracy: 0.0631\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.4706 - accuracy: 0.0811\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.4082 - accuracy: 0.0631\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.3358 - accuracy: 0.0631\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.2582 - accuracy: 0.0631\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.1739 - accuracy: 0.0631\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.0881 - accuracy: 0.0631\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 4.0022 - accuracy: 0.0631\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 3.9231 - accuracy: 0.0631\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 3.8486 - accuracy: 0.0631\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 3.7831 - accuracy: 0.0631\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 3.7224 - accuracy: 0.0631\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 3.6722 - accuracy: 0.0631\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 3.6280 - accuracy: 0.0631\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c40280505e0>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= np.transpose(model.layers[2].get_weights()[0])"
      ],
      "metadata": {
        "id": "NBCrp0HtQ9QB"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=np.array(a)"
      ],
      "metadata": {
        "id": "XY1fOEXmRAVu"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMAjHiJ6SkXW",
        "outputId": "992be2f1-47ef-40d0-873b-33e0196a8029"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(116, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[0]\n",
        "norm = np.linalg.norm(a[0])\n",
        "normalized_a1 = a[0] / norm\n"
      ],
      "metadata": {
        "id": "OhG2M6F1UBne"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_a1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmWWd5lFWwWo",
        "outputId": "0af5f007-ef0e-4b20-9572-c1463e898538"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.6720861, -0.740473 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[2]\n",
        "norm = np.linalg.norm(a[2])\n",
        "normalized_a12 = a[2] / norm\n"
      ],
      "metadata": {
        "id": "rdi0Z3eKSrZR"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay8CngZsT-JB",
        "outputId": "a7e5cf38-5a85-487f-d794-821b109ce3ee"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03420541, 0.3743359 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_a12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyN3yfs7W-hO",
        "outputId": "bcce38f0-cf39-40f2-c1a6-9a32612e676b"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.75897884, -0.6511152 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute dot product\n",
        "dot_product = np.dot(normalized_a1, normalized_a12)\n",
        "\n",
        "# Print the result\n",
        "print(\"Dot product:\", dot_product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-o_pHRvXMU3",
        "outputId": "e38816c2-676c-4ddb-e836-9f79eef7d8ee"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot product: 0.9922323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[115]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UksAjgVKUAHt",
        "outputId": "db43c112-e3ce-42ad-d527-f04d16f030a9"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.73012865, -0.51395357], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'a' is your weight matrix obtained as per your example\n",
        "a = np.transpose(model.layers[2].get_weights()[0])  # Transpose if needed to match your matrix shape\n",
        "\n",
        "# Function to normalize each row\n",
        "def normalize_rows(matrix):\n",
        "    norms = np.linalg.norm(matrix, axis=1, keepdims=True)  # Compute L2 norm of each row\n",
        "    return matrix / norms  # Normalize each row by its L2 norm\n",
        "\n",
        "# Normalize the weight matrix 'a' along axis 0 (rows)\n",
        "normalized_a = normalize_rows(a)\n",
        "\n",
        "# Print the normalized matrix\n",
        "print(\"Normalized matrix:\")\n",
        "print(normalized_a[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUWoThqcUHR-",
        "outputId": "58c479bf-5ab8-4372-f725-c2e53f642819"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized matrix:\n",
            "[0.09099715 0.99585116]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "XNo71-O-YWs6"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [normalized_a[idx - 1] for idx in sorted(tokenizer.word_index.values())]"
      ],
      "metadata": {
        "id": "oJYXrGFxZeSP"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def calculate_cosine_similarity(embeddings):\n",
        "    num_words = len(embeddings)\n",
        "    similarities = np.zeros((num_words, num_words))\n",
        "\n",
        "    # Check if the output file exists, create it if not\n",
        "    if not os.path.exists(output_file):\n",
        "        with open(output_file, 'w'):  # Create the file\n",
        "            pass\n",
        "\n",
        "    with open(output_file, 'a') as file:\n",
        "        for i in range(num_words):\n",
        "            for j in range(num_words):\n",
        "                if i != j:\n",
        "                    similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
        "                    similarities[i, j] = similarity\n",
        "                    word1 = list(tokenizer.word_index.keys())[i]\n",
        "                    word2 = list(tokenizer.word_index.keys())[j]\n",
        "                    file.write(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\\n\")\n",
        "\n",
        "    return similarities\n"
      ],
      "metadata": {
        "id": "9CAKvP6_aDuJ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between all pairs of words\n",
        "output_file = 'cosine_similarities.txt'\n",
        "similarities = calculate_cosine_similarity(embeddings)"
      ],
      "metadata": {
        "id": "92Wbw2qVaOK7"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N8Z8RBxfaRPz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}